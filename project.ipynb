{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Answering the Question**\n",
        "## **Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data?**\n",
        "We're trying to predict the ordinal variable damage_grade, which represents a level of damage to the building that was hit by the earthquake. There are 3 grades of the damage:\n",
        "1 represents low damage\n",
        "2 represents a medium amount of damage\n",
        "3 represents almost complete destruction\n",
        "\n",
        "## **Did you define the metric for success before beginning?**\n",
        "To measure the performance of our algorithms, we'll use the F1 score which balances the precision and recall of a classifier. Traditionally, the F1 score is used to evaluate performance on a binary classifier, but since we have three possible labels we will use a variant called the micro averaged F1 score.\n",
        "## **Did you understand the context for the question and the scientific or business application?**\n",
        "Predict the level of damage to buildings caused by the 2015 Gorkha earthquake to help predict the damages of future earthquakes.\n",
        "## **Did you record the experimental design?**\n",
        "Throughout this process, the National Planning Commission, along with Kathmandu Living Labs and the Central Bureau of Statistics, has generated one of the largest post-disaster datasets ever collected, containing valuable information on earthquake impacts, household conditions, and socio-economic-demographic statistics.\n",
        "## **Did you consider whether the question could be answered with the available data?**\n",
        "The question should be able to be answered by the dataset.\n"
      ],
      "metadata": {
        "id": "2rhneUtZzoEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 & 3: Checking and Tidying the Data**"
      ],
      "metadata": {
        "id": "9G7BEgxi0iIr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rA65plGmqxFs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_vals = pd.read_csv(\"train_values.csv\")\n",
        "dataset_labs = pd.read_csv(\"train_labels.csv\")\n",
        "dataset = dataset_vals.merge(dataset_labs)\n",
        "dataset = dataset.dropna(axis = 0)\n",
        "dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQrQT7HOq4Ht",
        "outputId": "ffae35f1-435e-4dd7-a2b8-f68409dfb418"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 46616 entries, 0 to 46615\n",
            "Data columns (total 40 columns):\n",
            " #   Column                                  Non-Null Count  Dtype  \n",
            "---  ------                                  --------------  -----  \n",
            " 0   building_id                             46616 non-null  int64  \n",
            " 1   geo_level_1_id                          46616 non-null  int64  \n",
            " 2   geo_level_2_id                          46616 non-null  int64  \n",
            " 3   geo_level_3_id                          46616 non-null  int64  \n",
            " 4   count_floors_pre_eq                     46616 non-null  int64  \n",
            " 5   age                                     46616 non-null  int64  \n",
            " 6   area_percentage                         46616 non-null  int64  \n",
            " 7   height_percentage                       46616 non-null  int64  \n",
            " 8   land_surface_condition                  46616 non-null  object \n",
            " 9   foundation_type                         46616 non-null  object \n",
            " 10  roof_type                               46616 non-null  object \n",
            " 11  ground_floor_type                       46616 non-null  object \n",
            " 12  other_floor_type                        46616 non-null  object \n",
            " 13  position                                46616 non-null  object \n",
            " 14  plan_configuration                      46616 non-null  object \n",
            " 15  has_superstructure_adobe_mud            46616 non-null  int64  \n",
            " 16  has_superstructure_mud_mortar_stone     46616 non-null  int64  \n",
            " 17  has_superstructure_stone_flag           46616 non-null  int64  \n",
            " 18  has_superstructure_cement_mortar_stone  46616 non-null  int64  \n",
            " 19  has_superstructure_mud_mortar_brick     46616 non-null  int64  \n",
            " 20  has_superstructure_cement_mortar_brick  46616 non-null  int64  \n",
            " 21  has_superstructure_timber               46616 non-null  float64\n",
            " 22  has_superstructure_bamboo               46616 non-null  float64\n",
            " 23  has_superstructure_rc_non_engineered    46616 non-null  float64\n",
            " 24  has_superstructure_rc_engineered        46616 non-null  float64\n",
            " 25  has_superstructure_other                46616 non-null  float64\n",
            " 26  legal_ownership_status                  46616 non-null  object \n",
            " 27  count_families                          46616 non-null  float64\n",
            " 28  has_secondary_use                       46616 non-null  float64\n",
            " 29  has_secondary_use_agriculture           46616 non-null  float64\n",
            " 30  has_secondary_use_hotel                 46616 non-null  float64\n",
            " 31  has_secondary_use_rental                46616 non-null  float64\n",
            " 32  has_secondary_use_institution           46616 non-null  float64\n",
            " 33  has_secondary_use_school                46616 non-null  float64\n",
            " 34  has_secondary_use_industry              46616 non-null  float64\n",
            " 35  has_secondary_use_health_post           46616 non-null  float64\n",
            " 36  has_secondary_use_gov_office            46616 non-null  float64\n",
            " 37  has_secondary_use_use_police            46616 non-null  float64\n",
            " 38  has_secondary_use_other                 46616 non-null  float64\n",
            " 39  damage_grade                            46616 non-null  int64  \n",
            "dtypes: float64(17), int64(15), object(8)\n",
            "memory usage: 14.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding independent variable\n",
        "objList = dataset.select_dtypes(include = 'object').columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "for col in objList:\n",
        "    dataset[col] = le.fit_transform(dataset[col].astype(str))\n",
        "X = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1].values\n",
        "#Encoding dependent variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y) "
      ],
      "metadata": {
        "id": "aeZAn2lbrg4n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3)\n",
        "#feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "I9XWXZNjs9ut"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4: Exploratory analysis**"
      ],
      "metadata": {
        "id": "xB9h6PX105tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_palette('colorblind')\n",
        "sns.pairplot(data=dataset, height=3)"
      ],
      "metadata": {
        "id": "QIAb0lIIr49S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the decision tree classification model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "#making a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(cm) #rows actuals, columns predictions\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hxp53K_0t8vW",
        "outputId": "33c76a87-50fb-4d3e-ea9a-ae5b74e1462e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3518  3398   569]\n",
            " [ 3514 31121 10248]\n",
            " [  483  9701 15629]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6429695194484594"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression(random_state = 0)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "#making a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "print(cm) #rows actuals, columns predictions\n",
        "accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67EeCBPzsREz",
        "outputId": "045a6c55-66b8-4acf-828e-fef9a21b8978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2075  5266   144]\n",
            " [ 1543 40452  2888]\n",
            " [  136 22345  3332]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.586574743224057"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}